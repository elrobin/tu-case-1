---
title: "Unveiling the ecosystem of science: Towards an integrative valuation model of the many roles of scientists"
author: 
  - name: Nicolas Robinson-Garcia
    email: N.Robinson@tudelft.nl
    affiliation: a
    footnote: Corresponding author
  - name: Rodrigo Costas
    affiliation: b
  - name: Thed N. van Leeuwen
    affiliation: b
  - name: Tina Nane
    affiliation: a
address:
  - code: a
    address: Applied Mathematics (DIAM), TU Delft, Delft, Netherlands
  - code: b
    address: CWTS, Leiden University, Leiden, Netherlands
abstract: There is increasing evidence on the misuse and abuse of quantitative indicators in the current scientific reward system. Alternatively, more qualitative approaches, use of case studies or the design of indicators more sensitive to societal and scientific needs have been suggested. In this article we analyze the bases of such criticisms and motivations for changing the reward system by focusing on the assessment of individuals. We explore alternative models proposed or in use and identify common characteristics. Based on this we propose a valuation model by which we can systematically organize and prioritize performative aspects of scientists and consider other factors which may affect or might relevant for research policy. We finally test our model in a series of case studies based on six academic units.
output: rticles::elsevier_article
bibliography: references.bib
---

# 1. Introduction

The ecosystem of science is defined as a system with interconnected entities integrated in a larger social system with which a bidirectional influence is exerted [@fortunatoScienceScience2018]. Scientists, as drivers of the research enterprise, are immersed in an increasingly diverse set of tasks and activities with the purpose of producing, communicating and transferring knowledge to society. This involves a series of skills which go beyond their intellectual or technical capabilities (e.g., negotiation skills, communication skills, social engagement). The introduction of national and supranational research evaluation systems [@hicksPerformancebasedUniversityResearch2012] carved the path to introducing measures of assessment in the hope that they would help monitor and maximize such research efforts. While peer review has  traditionally been the main mechanism of quality control [@bornmannScientificPeerReview2011], there has been an increasing use of bibliometric indicators. According to @glaserImpactEvaluationbasedFunding2002, this is due to the fact that,

> "[b]y applying quantitative performance indicators, actors in science can be compared according to their performance more or less independent of peers’ judgments, and science policy can reach conclusions seemingly independent of the scientific community to which an actor belongs" (pp. 29).

An overemphasis on quantitative  and specifically, bibliometric indicators, has led to a situation in which academic careers are assessed and rewarded relying solely on individuals' capacity to publish highly cited research articles in highly ranked journals [@mckiernanUseJournalImpact2019; @woutersCitationCultureInfrastructure2014]. This unidimensional view of the research enterprise has been harshely criticized [@doraSanFranciscoDeclaration2014; @hicksLeidenManifestoResearch2015; @wilsdonMetricTideReport2015], arguing mostly but not exclusively, on the methodological and conceptual limitations they have when assessing individuals. But there are more profound and serious effects derived from current research evaluation schemes. 

First, the focus on publications may lead to a task reduction, where scientists neglect those types of work which are not recognized [@rijckeEvaluationPracticesEffects2016]. As consequence, there is a goal misplacement as the researcher's efforts go into gaining higher scores in the measures instead of pursuing their own research agenda. Second, quantitative indicators are not always adequately used and introduced in research evaluation schemes [@rafolsDominanceQuantitativeEvaluation2016], which means that seemingly 'egalitarian' criteria can greatly hamper researchers from certain fields or specific profiles [@glaserEvaluationEvaluators2007]. Also, because such indicators can be gamed [@fisterDiscoveryCitationCartels2016], counteracting rules to avoid misbehaviours can be even more harming [@robinson-garciaTieneSentidoLimitar2018]. Third the focus on publishing in journals with a high impact factor in tenure and promotion [@mckiernanUseJournalImpact2019] leads to systematic biases on the type of scientific knowledge produced. @rafolsHowJournalRankings2012 demonstrated systematic suppressing of interdisciplinary research when using journal-based indicators for research evaluation. @pineiroReceptionSpanishSociology2015 showed that, in the case of Spanish Sociology, policies promoting publication in high impact journals indexed in the Web of Science would penalize locally-oriented research. @chavarroWhyResearchersPublish2017 reported that non-mainstream journals, which play an important role in national science systems, are negatively affected by these policies.

Finally, because a single set of criteria is defined for all scholars, research evaluation schemes tend to promote a notion of the 'excellent scientist' who is capable of outperforming in all facets [@olmos-penuelaDoesItTake2016], threatening the diversity of scientific profiles researchers needed in the science ecosystem [@glaserImpactEvaluationbasedFunding2002; @woolley2014REFResults2017].

This article proposes an alternative model for evaluating scientists' performance. This approach balances between a conceptually-informed framework and a methodologically viable operationalization. We first provide evidences on the specific aspects that are being negatively affected by research evaluation schemes by reviewing previous empirical work. After reviewing the different proposals made in the literature, we propose a set of five performative dimensions which consider as policy-relevant when conducting research assessment of individuals. A key aspect of the model has to do with the networked nature of scientific work and hence, the heterogeneity of research profiles developed. Hence, how research trajectories develop and understanding how specialization on specific roles might affect such categories is a key point for our model. Also external factors affecting individuals' performance along with effects of evaluation on personal features such as gender, nationality or age, are considered as they can be directly or indirectly affected by research policies and evaluation schemes. The model is  implemented in a multiple-case study of six organisational subunits (i.e., departments, laboratories) from two Dutch universities. This work contributes to existing research on the scientific workforce and research careers in an inefficient evaluative context [@hammarfeltIndicatorsJudgmentDevices2017] which threatens diversity of scientific profiles, and hence of outputs and impacts [@olmos-penuelaDoesItTake2016; @woolley2014REFResults2017].

# 2. Effects of evaluation on research careers

**Claim 2: Publications offer a partial view of researchers' outcomes.**

-	What do researchers do?
-	How they organize their time?
-	Which are their outcomes?

**Claim 3: Invisible profiles are becoming more necessary than ever but are being kicked out of the system.**

-	How do they distribute tasks?
-	How do they negotiate authorship?
-	How do they allocate credit and prestige
-	Supporting evidence: Milojevic et al. PNAS (2018) publication.

# 2. Lit review

## Stratification and diversity of scientific profiles

Examples of dimensions can be given from the book by Bastow, Tinkler & Dunleavy (2014)

Bozeman, Dietz & Gaughan (2001) present their evaluation model at the individual level as follows:

-	Internal resources: cognitive skills (this can be criticized as it is impossible to operationalize as they present it i.e., ability to synthesize), S&T knowledge and context skills (learnt through experience). The authors indicate that each resource may have n dimensions.
-	S&T Capital: Which has to do with the network of scientists and the intrinsic value they have as well as the role the individual plays in such network
-	S&T Human Capital and Life Cycles: The authors recognize that researchers' profile will be different at different stages of their career.
-	Corley et al (2017) introduce a cultural dimension to the model which refers to variables such as gender, nationality, race, discipline or socio-economic status. 

ACUMEN presents a portfolio, a kind of CV format designed for assessing individual performance. It combines qualitative and quantitative information. It offers space for a narrative where an individual tells her own story. It then distinguishes between three aspects of an academic's career: expertise (methods, areas of theory, etc.), outputs (publications, patents, etc.) and impacts (citations, awards, etc.). Furthermore, it includes an individual's age. For these three aspects it includes quantitative indicators. The perspective here is that individuals report their CV and the ACUMEN portfolio basically structures and offers recommendations on how this should be provided. Other than that it does not give clear indications on how the assessment should be performed, it seems to rely on experts' judgment.

Look into Whitley

Look into Evaluative Inquiry

### Effects of evaluation on research careers: Some evidences


| Critique | Evidences | Reference|
|----------|-----------|----------|
|Abuse of JIF in evaluations| Documentation from 129 univs. and 381 academic units in US and Canada | McKiernan et al., 2019|
|Misalignment between societal needs and research | Desktop research and expert panel workshop | Moher et al., 2018 |
|Bibliometric indicators pervert scientific enterprise| Policy experience | Benedictus (2016), Edwards (2017)|

Authors like Moher et al. (2018) or Nosek (2015) refer to research integrity, and in the case of the latter, refer to conflict of interest of researchers as they might be interested on publication even if results are not accurate. This would lead to Ioannidis' famous paper on the reproducibility crisis and false results.

Abramo, G., D'Angelo, C. A., & Rosati, F. (2015). The determinants of academic career advancement: Evidence from Italy. Science and Public Policy, 42(6), 761-774. https://doi.org/10.1093/scipol/scu086

Luukkonen, T., & Thomas, D. A. (2016). The 'Negotiated Space' of University Researchers' Pursuit of a Research Agenda. Minerva, 54(1), 99-127. https://doi.org/10.1007/s11024-016-9291-z

### Career trajectories
Stephan & Levin -> In terms of productivity peaks and decline

Laudel, G., Bielick, J., & Gläser, J. (2018). 'Ultimately the question always is: "What do I have to do to do it right?"' Scripts as explanatory factors of career decisions. Human Relations, 0018726718786550. https://doi.org/10.1177/0018726718786550

### Valuation models of scientific activity

I have changed the name of the section to valuation so that it alignes with the ISSI poster. This means that the variables considered in the table should also be modified and reflect on the aspects valued by each of these models.

Interesting the model presented by Boyer (1990) and used in the paper by Herman & Nicholas (2019) published in El profesional de la información. Furthermore, should consider the term *facet* instead of referring to *dimensions*, it might be more accurate.

| Evaluation model |	Output/outcome |	Unit of analysis | References |
|------------------|-----------------|-------------------|------------|
|Bibliometric assessment |	Production and impact	| Scalable | |
|Economic analysis |                 | | |
|Knowledge value alliances | |	Communities of scholars (e.g., labs) | Bozeman & Rogers |
|Research portfolios | Journal articles? | | Wallace & Rafols, 2015 |
|Productive interactions | Process-oriented | Research projects | Spaape & van Drooge, 2011 |
|S&T Human Capital | | Scalable | Bozeman, Dietz & Gaughan, 2001 |

# 3. Methodological framework

## Data

To test the reliability of the model, we selected a technical university and a general university. Furthermore, these subunits belong to three different fields (Medical Sciences, Engineering Sciences and Social Sciences), a

## Evaluative dimensions

### Scientific engagement
Collaboration ties and diversity (number of unique collaborators and intensity), position in network, (co-authorship, acknowledgment, etc.), strength of tie?, interdisciplinarity, application of research, member of committees, reviewer, etc.

Social engagement

Non-academic collaboration ties (either through publications or not i.e., reference patterns), some altmetric indicators (policy briefs but also maybe twitter classes? Or potentially Ed's ABC score), spin-offs,

Capacity-building

Productivity, leadership, independence, scientific impact, funding

Trajectory/Background?

Geographic mobility, cognitive mobility, sectoral mobility (These last three could be seen not as attributes to the researcher but their capacity to work with diverse people), career status

Research practices (Cross-sectoral)

OA publications, data sharing, outreach (e.g., The Conversation, blogging, tweeting), publication of working papers, proceedings, mentoring

Some references

Alperin, J. P., Nieves, C. M., Schimanski, L., Fischman, G. E., Niles, M. T., & McKiernan, E. C. (2018). How significant are the public dimensions of faculty work in review, promotion, and tenure documents? Recuperado de https://hcommons.org/deposits/item/hc:21015/

July 3rd, researchers, 2018|Early career, education, H., science, O., evaluation, R., & Comment, R. policy|1. (2018, julio 3). Making research evaluation processes in Europe more transparent. Recuperado 20 de febrero de 2019, de https://blogs.lse.ac.uk/impactofsocialsciences/2018/07/03/making-research-evaluation-processes-in-europe-more-transparent/

Schimanski, L. A., & Alperin, J. P. (2018). The evaluation of scholarship in academic promotion and tenure processes: Past, present, and future. F1000Research, 7, 1605. https://doi.org/10.12688/f1000research.16493.1

### Confounding effects

### Personal features

Order dimensions from time independent to dependent.
Features within dimensions

# 4. Results

-	Showcase dimensions for research teams or labs -> Internal organization and division of labor
-	Showcase dimensions life cycles -> Different stages in career
-	Attribution Plos One -> Scientific engagement
-	Differences on profiles by fields.
-	Agencies

\pagebreak

# Codebook

Here I describe the selection of case studies and the data retrieval process and description of sources. The purpose of this is to show that 1) there are indeed a variety of different profiles, 2) these profiles co-exist and complement each other, 3) the diversity and typologies of profiles is field dependent, 4) personal features are key to understand this diversity. Here we emphasize two specific personal features: age and gender. Some of the research questions that could be answered descriptively are the following:

1. *Is there team science?*
  i. *Are these teams stable over time?* Some data that could be retrieved from WoS
  
    -	Cluster_id
    -	All classic bibliometric indicators
    -	Number of collaborators from same institution
    -	Number of external collaborators
    -	Number of collaborators from same institution by publication
    -	Number of external collaborators by publication
    
  ii. *Is there activity visible through co-authorship?* Check groups identified with manual checking in the web and maybe interviews

2. *Do research teams operate in a coordinated way?* Here it would be interesting to understand dependence relationship. Nederhof & van Raan (1993) refer to the star effects as what happens when a PI retires and the research group disappears. Here we should go beyond bibliometrics and see if there is someone in charge of the funding, someone of hiring and finding opportunities, someone who is more of a public face, etc. Also it would be interesting to use network analysis to determine authorities, hubs, etc. and contrast with their judgment. Some variables from WoS and interivews plus manually cheking: authorship position (WoS); acknowledgments data (WoS); clusters from Ludo's subject classification to identify areas of specialization per subject (WoS); social media activity; Google Scholar data.
  i. *Do they have a common research agenda?*
  ii. *How is this agenda established?*

3. *How does team science affect individual trajectories?*
  i. *How is credit shared?*
  ii. *What is the relation between the role exerted and academic status?* A cohort analysis?
  iii. *How is continuity of supporting scientists ensured?* Probably also from interviews. How do scholars change from institution or are maintained if they are not able to 'make the next step'.

## Selection of case studies

The identification of research groups is done bibliometrically and based on Web of Science publications. For this I have selected all publications between 2008 and 2017 by LUMC, Leiden University or TU Delft. The following table includes some descriptives. Here I must note that researchers belonging to institutions are not based on the specific affiliation linkage of docs (which uses the Leiden Ranking affiliation already cleaned up), but based on `cluster_id` with either of the three institutions as their main or alternative address. This should be checked to see if there is a way to link to the `cluster_id` organizations to the cleaned affiliations from Leiden Ranking. I had to clean up this data myself. In any case this should not be a concern for the selection of case studies.


|              | TU Delft     | Leiden Univ | LUMC   |
|--------------|:------------:|:-----------:|:------:|
|Publications  | 24,233       | 49,149      | 3,188  |
|Researchers   | 9,975        | 14,116      | 279    |
|Collaborators | 34,409       | 117,307     | 14,153 |
|Mean au/p     | 4.7          | 9.6         | 10.1   |
|Median au/p   | 4            | 6           | 8      |
|Sd au/p       | 5.4          | 31.5        | 18.7   |

The next figure shows the distribution of papers based on the number of authors by paper (A) and the thematic profile of each of the three institutions. Leiden University is the largest of the three institutions with a more comprehensive portfolio, although mostly focues on Biomedical Sciences and Social Sciences. This focus on biomedical sciences is obviously more noticeable in the case of LUMC, although it still has some pubications in fields of the social sciences, mostly related with Public Health. Finally, TU Delft shows a profile focused on Phisics, Engineering and Mathematics. While there might be an overlap between LUMC and Leiden University, the high preponderance of biomedical literature might also be due to a close relation between these universities.

```{r echo=FALSE}
knitr::include_graphics("figs/histogram-profiles.png", dpi = 300)
```

Six case studies will be selected. Three for each university and two by field. The purpose of this is not only to identify differences by discipline but also by institutional type. The fields are:

1. Physics and Engineering
2. Social Sciences and Humanities
3. Biomedical Sciences

Following I include the collaboration networks for each university and field. I have included a threshold of at least 10 publications by `cluster_id`, filtered by the giant component and calculated the betweenness centrality of each node. I have selected as seed researcher the one with the highest centrality.

### Physics & Engineering
#### 1. TU Delft

```{r echo=FALSE}
knitr::include_graphics("figs/tu_phys_betweenness.png", dpi = 300)
```

*Notes:*

- 1260 nodes (21.9% visible); 4702 edges (27.1% visible). 
- `cluster_id` with highest betweenness = 33800547; Betweenness centrality: 0.11; Total publications = 159; Age = 32
- Name: Frans D. Tichelaar; First year: 1986; Last year: 2018
- PURE: https://pure.tudelft.nl/portal/en/persons/fd-tichelaar(56299c58-b6ec-478b-b188-b8744b69d954).html
- Institution: http://nchrem.nl/people/dr-ir-f-d-tichelaar-frans/

#### 2. Leiden Univ

```{r echo=FALSE}
knitr::include_graphics("figs/lu_phys_betweenness.png", dpi = 300)
```

*Notes:*

- 765 nodes (35.1% visible); 3409 edges (40.6% visible). 
- `cluster_id` with highest betweenness = 25501410; Betweenness centrality: 0.23; Total publications = 590; Age = 38
- Name: Ewine F. van Dishoeck; First year: 1980; Last year: 2018
- Institution: https://local.strw.leidenuniv.nl/people/touchscreen2/persinline.php?id=16
- Personal website: https://home.strw.leidenuniv.nl/~ewine/

### Biomedical and Health Sciences

#### 1. TU Delft

```{r echo=FALSE}
knitr::include_graphics("figs/tu_bio_betweenness.png", dpi = 300)
```

*Notes:*

- 214 nodes (18.9% visible); 615 edges (25.49% visible). 
- `cluster_id` with highest betweenness = 43204348; Betweenness centrality: 0.50; Total publications = 341; Age = 31
- Name: Harrie H. Weinans; First year: 1987; Last year: 2018
- Google Profile: https://scholar.google.com/citations?user=di4NUp8AAAAJ&hl=en
- PURE: https://pure.tudelft.nl/portal/en/persons/hh-weinans(f31bd75b-1863-4202-b64b-7356538284a7)/publications.html
- Co-affiliated to UMC Utrecht and TU Delft.

#### 2. Leiden Univ

```{r echo=FALSE}
knitr::include_graphics("figs/lu_bio_betweenness.png", dpi = 300)
```

*Notes:*

- Due to the density of the network the selected node can scarcely be seen.
- 3304 nodes (38.8% visible); 43647 edges (59.6% visible). 
- `cluster_id` with highest betweenness = 19936939; Betweenness centrality: 0.47; Total publications = 185; Age = 27
- Name: Ron Wolterbeek; First year: 1991; Last year: 2018
- Institution: https://www.lumc.nl/org/bds/medewerkers/rwolterbeek
- Affiliated to LUMC.

### Social Sciences and Humanities

#### 1. TU Delft

```{r echo=FALSE}
knitr::include_graphics("figs/tu_soc_betweenness.png", dpi = 300)
```

*Notes:*

- 151 nodes (12.6% visible); 281 edges (15.8% visible). 
- `cluster_id` with highest betweenness = 12392841; Betweenness centrality: 0.41; Total publications = 134; Age = 18
- Name: Bert van Wee; First year: 1999; Last year: 2018
- Google Profile: https://scholar.google.es/citations?user=dYUiqMYAAAAJ&hl=en
- Institution: https://www.tudelft.nl/en/tpm/about-the-faculty/departments/engineering-systems-and-services/people/full-professors/profdr-gp-bert-van-wee/

#### 2. Leiden Univ

```{r echo=FALSE}
knitr::include_graphics("figs/lu_soc_betweenness.png", dpi = 300)
```

*Notes:*

In this case, the selection of the seed researcher was not based on network indicator. The network above has the OpenOrd layout (instead of Yi ) and K-Core=1 group. The main issue here is that this field is largely populated by biomedical scientists and psychologists and psychiatrists, fields which are not good representations of Social Sciences and Humanities. What I have done is looked at those pairs of scholars with the highest shares of co-authored papers and go down the list until I found someone who was not from these fields nor from CWTS (Ludo and Nees are the third pair with more co-authored papers)

- 478 nodes (26.5% visible); 1294 edges (43.1% visible). 
- `cluster_id` selected = 36871407; Betweenness centrality: 0.00; Total publications = 78; Age = 18
- Name: Judi Mesman; First year: 2000; Last year: 2018
- Institution: https://www.universiteitleiden.nl/en/staffmembers/judi-mesman/publications#tab-1
- Lab1: http://www.diversityinparenting.nl/
- Lab2: https://www.societalchallengeslab.com/

## Expansion from seed to complete team

Based on the six individuals selected in the first phase. I know search for their complete research team. The data retrieval process started on  March, 2019. Here I include for each case how I have proceeded.

### Physics & Engineering - TU Delft

Dr. Ir. F.D. Tichelaar belongs to the National Centre for High Resolution Electron Microscopy. According to its website, he is not the head of the institute which is formed by 9 researchers (5 staff and 4 researchers and post docs)

### Physics & Engineering - Leiden Univ

Prof. Ewine van Dishoeck belongs to the Leiden Observatory. According to its website, there are 179 workers: 33 are staff members, 50 postdocs, 63 PhD students and 26 supporting staff.

### Biomedical Sciences - TU Delft

Harrie H. Weinans is associate professor at UMC Utrecht in the department of Orthopaedics and Professor at TU Delft at the Biomechanical Engineering department. Here information is gathered from TU Delft. He is a parttime professor. Here I will not include the whole department, but only researchers working on the Biomaterials & Tissue Biomechanics group. Only supporting staff assigned to this group is included.

### Biomedical Sciences - Leiden University

Ron Wolterbeek is first-line consultant in the area of Medical Statistics at LUMC. I could not find him actually in the staff page, so decided to look into the staff included there. There is not much about him in the Internet other than his publications.

### Social Sciences and Humanities - TU Delft

Bert van Wee is professor in Transport Policy at the department of Engineering Systems and Services. In this case I do not find anything as research teams or groups and hence I am including the whole department.

### Social Sciences and Humanities - Leiden Univ

Judi Mesman is dean of Leiden University College The Hague and professor of the interdisciplinary study of societal challenges. She directs the Societal Challenges Lab which is part of both the Faculty of Governance and global Affairs and the Faculty of Social Sciences at Leiden University. This case study is based on her lab.

## Data sources

Data sources are selected based on dimensions from the *valuation model*. Following I include the main ones selected:

- **CWTS-Web of Science.** Publication and citation indicators are retrieved since 1980 or first publication to 2018 by matching individual to `cluster_id` following the author name disambiguation developed by Caron & van Eck (2014).
- 