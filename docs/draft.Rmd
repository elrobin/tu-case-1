---
title: "Unveiling the ecosystem of science: Towards an integrative valuation model of the many roles of scientists"
author: 
  - name: Nicolas Robinson-Garcia
    email: N.Robinson@tudelft.nl
    affiliation: a
    footnote: Corresponding author
  - name: Rodrigo Costas
    affiliation: b
  - name: Thed N. van Leeuwen
    affiliation: b
  - name: Tina Nane
    affiliation: a
address:
  - code: a
    address: Applied Mathematics (DIAM), TU Delft, Delft, Netherlands
  - code: b
    address: CWTS, Leiden University, Leiden, Netherlands
abstract: There is increasing evidence on the misuse and abuse of quantitative indicators in the current scientific reward system. Alternatively, more qualitative approaches, use of case studies or the design of indicators more sensitive to societal and scientific needs have been suggested. In this article we analyze the bases of such criticisms and motivations for changing the reward system by focusing on the assessment of individuals. We explore alternative models proposed or in use and identify common characteristics. Based on this we propose a valuation model by which we can systematically organize and prioritize performative aspects of scientists and consider other factors which may affect or might relevant for research policy. We finally test our model in a series of case studies based on six academic units.
output: rticles::elsevier_article
bibliography: references.bib
---

# 1. Introduction

The ecosystem of science is defined as a system with interconnected entities integrated in a larger social system with which a bidirectional influence is exerted [@fortunatoScienceScience2018]. Scientists, as drivers of the research enterprise, are immersed in an increasingly diverse set of tasks and activities with the purpose of producing, communicating and transferring knowledge to society. This involves a series of skills which go beyond their intellectual or technical capabilities (e.g., negotiation skills, communication skills, social engagement). The introduction of national and supranational research evaluation systems [@hicksPerformancebasedUniversityResearch2012] carved the path to introducing measures of assessment in the hope that they would help monitor and maximize such research efforts. While peer review has  traditionally been the main mechanism of quality control [@bornmannScientificPeerReview2011], there has been an increasing use of bibliometric indicators. According to @glaserImpactEvaluationbasedFunding2002, this is due to the fact that,

> "[b]y applying quantitative performance indicators, actors in science can be compared according to their performance more or less independent of peers' judgments, and science policy can reach conclusions seemingly independent of the scientific community to which an actor belongs" (pp. 29).

An overemphasis on quantitative  and specifically, bibliometric indicators, has led to a situation in which academic careers are assessed and rewarded relying solely on individuals' capacity to publish highly cited research articles in highly ranked journals [@mckiernanUseJournalImpact2019; @woutersCitationCultureInfrastructure2014]. This unidimensional view of the research enterprise has been harshely criticized [@doraSanFranciscoDeclaration2014; @hicksLeidenManifestoResearch2015; @wilsdonMetricTideReport2015], arguing mostly but not exclusively, on the methodological and conceptual limitations they have when assessing individuals. But there are more profound and serious effects derived from current research evaluation schemes. 

First, the focus on publications may lead to a task reduction, where scientists neglect those types of work which are not recognized [@rijckeEvaluationPracticesEffects2016]. As consequence, there is a goal misplacement as the researcher's efforts go into gaining higher scores in the measures instead of pursuing their own research agenda. Second, quantitative indicators are not always adequately used and introduced in research evaluation schemes [@rafolsDominanceQuantitativeEvaluation2016], which means that seemingly 'egalitarian' criteria can greatly hamper researchers from certain fields or specific profiles [@glaserEvaluationEvaluators2007]. Also, because such indicators can be gamed [@fisterDiscoveryCitationCartels2016], counteracting rules to avoid misbehaviours can be even more harming [@robinson-garciaTieneSentidoLimitar2018]. Third the focus on publishing in journals with a high impact factor in tenure and promotion [@mckiernanUseJournalImpact2019] leads to systematic biases on the type of scientific knowledge produced. @rafolsHowJournalRankings2012 demonstrated systematic suppressing of interdisciplinary research when using journal-based indicators for research evaluation. @pineiroReceptionSpanishSociology2015 showed that, in the case of Spanish Sociology, policies promoting publication in high impact journals indexed in the Web of Science would penalize locally-oriented research. @chavarroWhyResearchersPublish2017 reported that non-mainstream journals, which play an important role in national science systems, are negatively affected by these policies.

Finally, because a single set of criteria is defined for all scholars, research evaluation schemes tend to promote a notion of the 'excellent scientist' who is capable of outperforming in all facets [@olmos-penuelaDoesItTake2016], threatening the diversity of scientific profiles researchers needed in the science ecosystem [@glaserImpactEvaluationbasedFunding2002; @woolley2014REFResults2017].

This article proposes an alternative model for evaluating scientists' performance. This approach balances between a conceptually-informed framework and a methodologically viable operationalization. We first provide evidences on the specific aspects that are being negatively affected by research evaluation schemes by reviewing previous empirical work. After reviewing the different proposals made in the literature, we propose a set of five performative dimensions which consider as policy-relevant when conducting research assessment of individuals. A key aspect of the model has to do with the networked nature of scientific work and hence, the heterogeneity of research profiles developed. Hence, how research trajectories develop and understanding how specialization on specific roles might affect such categories is a key point for our model. Also external factors affecting individuals' performance along with effects of evaluation on personal features such as gender, nationality or age, are considered as they can be directly or indirectly affected by research policies and evaluation schemes. The model is  implemented in a multiple-case study of six organisational subunits (i.e., departments, laboratories) from two Dutch universities. This work contributes to existing research on the scientific workforce and research careers in an inefficient evaluative context [@hammarfeltIndicatorsJudgmentDevices2017] which threatens diversity of scientific profiles, and hence of outputs and impacts [@olmos-penuelaDoesItTake2016; @woolley2014REFResults2017].

# 2. Pervasive effects attributed to the scientific reward system

Bibliometric indicators were incorporated in research evaluation exercises in the early 1970s [@debellisHistoryEvolutionBiblio2014]. Based on the notion of citation and productivity as predictors of recognition and academic success [@reskinScientificProductivityReward1977; @mertonMatthewEffectScience1968], their use soon expanded to from the United States to Europe through the United Kingdom's first national Research Assessment Exercises in the 1980s [@luukkonenResearchEvaluationEurope2002]. Citation and publication-based indicators were seen as an objective and measurable way in which to assess performativity and impact. Merton's normative framework of recognition built the grounds for the use of citations and proxies for research quality. But it ignored the effects the use of such indicators would have on scientists behavior and consequently, on the production of novel societally relevant scientific knowledge [@benedictusFewerNumbersBetter2016]. Furthermore, scientific communication is drastically transforming due to the emergence of the Open Access movement and the development of social media and research infrastructure (i.e., data banks, repositories). This has also affected the business model behind the publishing industry (e.g., author fees). Hence, it is no longer feasible nor advisable to assess qualitatively research outputs nor rely on journal or journal-derived metrics solely [@ioannidisAssessingValueBiomedical2014].

## Effects on the scientific workforce

The first and most obvious unintended effect is  the emergence of the so-called _Publish or Perish_ culture. @dalenIntendedUnintendedConsequences2012 indicate that scientists working in high pressure environments perceive an external pressure to publish, which makes them pay excessive attention only to "tasks that benefit or glorify the individual (publishing internationally, being cited by other scholars)" instead of those "tasks that benefit larger groups" (pp. 1291). This leads to a task reduction [@rijckeEvaluationPracticesEffects2016] that negatively affect other tasks expected from scholars such as teaching, knowledge transfer [@lewisonSCICitationsNew2005] or engagement with societal stakeholders [@dalenIntendedUnintendedConsequences2012; @robinson-garciaUsingAlmetricsContextualised2018].

Second, funding opportunities affect how research is conducted [@luukkonenNegotiatedSpaceUniversity2016]. Academic freedom is mediated by the expectations the community has based on what it considers as scientific merit [@polanyiRepublicScienceIts2000]. As evaluation schemes shape such expectations, scientists' goals can deviate from what is considered more relevant or interesting, to what pays off best. By using journal rankings or rewarding high citation rates, evaluation schemes pressure scientists to modify their research agendas as well as their strategies to maximize their efforts. For instance, an emphasis on novelty is a contributing factor for the lack of replication studies, under-reporting of false positives, or testing of prior published results [@nosekScientificUtopiaII2012]. Also, strategic publishing can lead to select topics which are not necessarily societally relevant but that become scientific hypes, hence becoming very profitable in terms of citations [@rousseauHbubble2013].

Third, the notion of performativity becomes entangled with the indicators used to measure it. In this case, publication output and citation impact. A suspicion comes then from anything that deviates from the norm, not bein able to interpret correctly those cases, as "there may be favoritism in some competitions and smart hiring in others" [@abramoDeterminantsAcademicCareer2015, p. 772]. Furthermore, these policies tend to emphasize and promote the _superstar_ or leading excellent scientist [@olmos-penuelaDoesItTake2016], ignoring the increasing need for team players, also referred to as the 'middle author' [@mongeonRiseMiddleAuthor2017]. As competition instead of collaboration is promoted, those middle authors end up being excluded from the academic career [@milojevicChangingDemographicsScientific2018].

## Effects on knowledge production

| Critique | Evidences | Reference|
|----------|-----------|----------|
|Misalignment between societal needs and research | Desktop research and expert panel workshop | Moher et al., 2018 |


1. Research quality: Low quality, overproduction, lack of replication, research integrity, fraud, hypercompetition (less collaboration),

http://print-cfa.dk/wp-content/uploads/2019/06/ncrm-manchester-2019-jws.pdf

2. Transparency. P-hacking, reproducibility, under-reporting of negative results

Authors like Moher et al. (2018) or Nosek (2015) refer to research integrity, and in the case of the latter, refer to conflict of interest of researchers as they might be interested on publication even if results are not accurate. This would lead to Ioannidis' famous paper on the reproducibility crisis and false results.

3. Societal challenges: Not aligned with societal needs but with greater impact within scientific community.

4. Long-term agendas. Publish or perish culture is short-sighted.

# 3. Towards a valuation model on the many roles of scientists

**First include review on literature**
Examples of dimensions can be given from the book by Bastow, Tinkler & Dunleavy (2014)

Bozeman, Dietz & Gaughan (2001) present their evaluation model at the individual level as follows:

-	Internal resources: cognitive skills (this can be criticized as it is impossible to operationalize as they present it i.e., ability to synthesize), S&T knowledge and context skills (learnt through experience). The authors indicate that each resource may have n dimensions.
-	S&T Capital: Which has to do with the network of scientists and the intrinsic value they have as well as the role the individual plays in such network
-	S&T Human Capital and Life Cycles: The authors recognize that researchers' profile will be different at different stages of their career.
-	Corley et al (2017) introduce a cultural dimension to the model which refers to variables such as gender, nationality, race, discipline or socio-economic status. 

ACUMEN presents a portfolio, a kind of CV format designed for assessing individual performance. It combines qualitative and quantitative information. It offers space for a narrative where an individual tells her own story. It then distinguishes between three aspects of an academic's career: expertise (methods, areas of theory, etc.), outputs (publications, patents, etc.) and impacts (citations, awards, etc.). Furthermore, it includes an individual's age. For these three aspects it includes quantitative indicators. The perspective here is that individuals report their CV and the ACUMEN portfolio basically structures and offers recommendations on how this should be provided. Other than that it does not give clear indications on how the assessment should be performed, it seems to rely on experts' judgment.

Look into Whitley

Look into Evaluative Inquiry


**Career trajectories**
Stephan & Levin -> In terms of productivity peaks and decline

Laudel, G., Bielick, J., & Gläser, J. (2018). 'Ultimately the question always is: "What do I have to do to do it right?"' Scripts as explanatory factors of career decisions. Human Relations, 0018726718786550. https://doi.org/10.1177/0018726718786550

**Valuation models of scientific activity**

I have changed the name of the section to valuation so that it alignes with the ISSI poster. This means that the variables considered in the table should also be modified and reflect on the aspects valued by each of these models.

Interesting the model presented by Boyer (1990) and used in the paper by Herman & Nicholas (2019) published in El profesional de la información. Furthermore, should consider the term *facet* instead of referring to *dimensions*, it might be more accurate.

| Evaluation model |	Output/outcome |	Unit of analysis | References |
|------------------|-----------------|-------------------|------------|
|Bibliometric assessment |	Production and impact	| Scalable | |
|Economic analysis |                 | | |
|Knowledge value alliances | |	Communities of scholars (e.g., labs) | Bozeman & Rogers |
|Research portfolios | Journal articles? | | Wallace & Rafols, 2015 |
|Productive interactions | Process-oriented | Research projects | Spaape & van Drooge, 2011 |
|S&T Human Capital | | Scalable | Bozeman, Dietz & Gaughan, 2001 |

## Evaluative dimensions

### Scientific engagement
Collaboration ties and diversity (number of unique collaborators and intensity), position in network, (co-authorship, acknowledgment, etc.), strength of tie?, interdisciplinarity, application of research, member of committees, reviewer, etc.

### Social engagement

Non-academic collaboration ties (either through publications or not i.e., reference patterns), some altmetric indicators (policy briefs but also maybe twitter classes? Or potentially Ed's ABC score), spin-offs,

### Capacity-building

Productivity, leadership, independence, scientific impact, funding

### Trajectory/Background?

Geographic mobility, cognitive mobility, sectoral mobility (These last three could be seen not as attributes to the researcher but their capacity to work with diverse people), career status

### Research practices (Cross-sectoral)

OA publications, data sharing, outreach (e.g., The Conversation, blogging, tweeting), publication of working papers, proceedings, mentoring

Some references

Alperin, J. P., Nieves, C. M., Schimanski, L., Fischman, G. E., Niles, M. T., & McKiernan, E. C. (2018). How significant are the public dimensions of faculty work in review, promotion, and tenure documents? Recuperado de https://hcommons.org/deposits/item/hc:21015/

July 3rd, researchers, 2018|Early career, education, H., science, O., evaluation, R., & Comment, R. policy|1. (2018, julio 3). Making research evaluation processes in Europe more transparent. Recuperado 20 de febrero de 2019, de https://blogs.lse.ac.uk/impactofsocialsciences/2018/07/03/making-research-evaluation-processes-in-europe-more-transparent/

Schimanski, L. A., & Alperin, J. P. (2018). The evaluation of scholarship in academic promotion and tenure processes: Past, present, and future. F1000Research, 7, 1605. https://doi.org/10.12688/f1000research.16493.1

## Confounding effects

Luukkonen, T., & Thomas, D. A. (2016). The 'Negotiated Space' of University Researchers' Pursuit of a Research Agenda. Minerva, 54(1), 99-127. https://doi.org/10.1007/s11024-016-9291-z

Include here motivations of scientists -> puzzle, ribbon, gold and prosocial motivations.

## Personal features

Order dimensions from time independent to dependent.
Features within dimensions

Paper by Balen et al 2012

Also paper by Smith and Nosek 2015

# 4. Research design

## Sample

To test the reliability of the model, we selected a technical university and a general university. Furthermore, these subunits belong to three different fields (Medical Sciences, Engineering Sciences and Social Sciences), a

## Data sources

## Analyses

# 5. Results

Subsections are tentative

## Profiling of scientists

## Field specificities

## Career trajectories


Table with number of scholars in each lab by source (e.g., with Google Scholar profile) and by status.

- Scatterplots combining publications from different sources

-	Showcase dimensions for research teams or labs -> Internal organization and division of labor
-	Showcase dimensions life cycles -> Different stages in career
-	Attribution Plos One -> Scientific engagement
-	Differences on profiles by fields.
-	Agencies

# 6. Discussion

Tentative topics to discuss

- Diversity of roles or different career stages?

# References