@TechReport{mckiernanUseJournalImpact2019,
  title = {Use of the {{Journal Impact Factor}} in Academic Review, Promotion, and Tenure Evaluations},
  abstract = {The Journal Impact Factor (JIF) was originally designed to aid libraries in deciding which journals to index and purchase for their collections. Over the past few decades, however, it has become a relied upon metric used to evaluate research articles based on journal rank. Surveyed faculty often report feeling pressure to publish in journals with high JIFs and mention reliance on the JIF as one problem with current academic evaluation systems. While faculty reports are useful, information is lacking on how often and in what ways the JIF is currently used for review, promotion, and tenure (RPT). We therefore collected and analyzed RPT documents from a representative sample of 129 universities from the United States and Canada and 381 of their academic units. We found that 40\% of doctoral, research-intensive (R-type) institutions and 18\% of master's, or comprehensive (M-type) institutions explicitly mentioned the JIF, or closely related terms, in their RPT documents. Undergraduate, or baccalaureate (B-type) institutions did not mention it at all. A detailed reading of these documents suggests that institutions may also be using a variety of terms to indirectly refer to the JIF. Our qualitative analysis shows that 87\% of the institutions that mentioned the JIF supported the metric's use in at least one of their RPT documents, while 13\% of institutions expressed caution about the JIF's use in evaluations. None of the RPT documents we analyzed heavily criticized the JIF or prohibited its use in evaluations. Of the institutions that mentioned the JIF, 63\% associated it with quality, 40\% with impact, importance, or significance, and 20\% with prestige, reputation, or status. In sum, our results show that the use of the JIF is encouraged in RPT evaluations, especially at research-intensive universities, and indicates there is work to be done to improve evaluation processes to avoid the potential misuse of metrics like the JIF.},
  language = {en},
  number = {e27638v2},
  institution = {{PeerJ Inc.}},
  author = {Erin C. McKiernan and Lesley A. Schimanski and Carol Mu{\~n}oz Nieves and Lisa Matthias and Meredith T. Niles and Juan Pablo Alperin},
  month = {apr},
  year = {2019},
  file = {C:\\Users\\Usuario\\Zotero\\storage\\DJZG4MYZ\\McKiernan et al. - 2019 - Use of the Journal Impact Factor in academic revie.pdf;C:\\Users\\Usuario\\Zotero\\storage\\YPYH2MEU\\27638.html},
  doi = {10.7287/peerj.preprints.27638v2},
}

@InCollection{woutersCitationCultureInfrastructure2014,
  address = {{Cambridge, MA}},
  title = {The {{Citation}}: {{From Culture}} to {{Infrastructure}}},
  booktitle = {Beyond {{Bibliometrics}}: {{Harnessing Multidimensional Indicators}} of {{Scholarly Impact}}},
  publisher = {{MIT Press}},
  author = {Paul Wouters},
  editor = {Blaise Cronin and Cassidy R. Sugimoto},
  year = {2014},
  pages = {47-66},
}
@Article{hicksLeidenManifestoResearch2015,
  title = {The {{Leiden Manifesto}} for Research Metrics},
  volume = {520},
  issn = {0028-0836, 1476-4687},
  shorttitle = {Bibliometrics},
  number = {7548},
  journal = {Nature},
  doi = {10.1038/520429a},
  author = {Diana Hicks and Paul Wouters and Ludo Waltman and Sarah {de Rijcke} and Ismael Rafols},
  month = {apr},
  year = {2015},
  pages = {429-431},
}

@Misc{doraSanFranciscoDeclaration2014,
  title = {San {{Francisco}} Declaration on Research Assessment},
  howpublished = {https://sfdora.org/},
  author = {{DORA}},
  year = {2014},
}

@TechReport{wilsdonMetricTideReport2015,
  title = {The {{Metric Tide}}: {{Report}} of the {{Independent Review}} of the {{Role}} of {{Metrics}} in {{Research Assessment}} and {{Management}}.},
  institution = {{HEFCE}},
  author = {J. Wilsdon and Liz Allen and Eleonora Belfiore and Philip Campbell and Stephen Curry and Steven Hill and Richard Jones and Roger Kain and Simon Kerridge and Mike Thelwall and Jane Tinkler and Ian Viney and Paul Wouters and Jude Hill and Ben Johnson},
  year = {2015},
}
@Article{rijckeEvaluationPracticesEffects2016,
  title = {Evaluation Practices and Effects of Indicator Use\textemdash{}a Literature Review},
  volume = {25},
  issn = {0958-2029},
  abstract = {Abstract.  This review of the international literature on evaluation systems, evaluation practices, and metrics (mis)uses was written as part of a larger review},
  language = {en},
  number = {2},
  journal = {Research Evaluation},
  doi = {10.1093/reseval/rvv038},
  author = {Sarah {de Rijcke} and Paul F. Wouters and Alex D. Rushforth and Thomas P. Franssen and Bj{\"o}rn Hammarfelt},
  month = {apr},
  year = {2016},
  pages = {161-169},
  file = {C:\\Users\\Usuario\\Zotero\\storage\\QZ9GAQWJ\\Rijcke et al. - 2016 - Evaluation practices and effects of indicator useâ€”.pdf;C:\\Users\\Usuario\\Zotero\\storage\\2UQH83E3\\2362680.html},
}
@Article{glaserImpactEvaluationbasedFunding2002,
  title = {Impact of Evaluation-Based Funding on the Production of Scientific Knowledge: {{What}} to Worry about, and How to Find Out},
  volume = {31},
  shorttitle = {Impact of Evaluation-Based Funding on the Production of Scientific Knowledge},
  journal = {Expertise for the German Ministry for Education and Research},
  author = {Jochen Gl{\"a}ser and Grit Laudel and Sybille Hinze and Linda Butler},
  year = {2002},
  pages = {2002},
  file = {C:\\Users\\Usuario\\Zotero\\storage\\R9EDLQIT\\GlÃ¤ser et al. - 2002 - Impact of evaluation-based funding on the producti.pdf},
}
@Article{fortunatoScienceScience2018,
  title = {Science of Science},
  volume = {359},
  copyright = {Copyright \textcopyright{} 2018 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  issn = {0036-8075, 1095-9203},
  abstract = {The whys and wherefores of SciSci
The science of science (SciSci) is based on a transdisciplinary approach that uses large data sets to study the mechanisms underlying the doing of science\textemdash{}from the choice of a research problem to career trajectories and progress within a field. In a Review, Fortunato et al. explain that the underlying rationale is that with a deeper understanding of the precursors of impactful science, it will be possible to develop systems and policies that improve each scientist's ability to succeed and enhance the prospects of science as a whole.
Science, this issue p. eaao0185
Structured Abstract
BACKGROUNDThe increasing availability of digital data on scholarly inputs and outputs\textemdash{}from research funding, productivity, and collaboration to paper citations and scientist mobility\textemdash{}offers unprecedented opportunities to explore the structure and evolution of science. The science of science (SciSci) offers a quantitative understanding of the interactions among scientific agents across diverse geographic and temporal scales: It provides insights into the conditions underlying creativity and the genesis of scientific discovery, with the ultimate goal of developing tools and policies that have the potential to accelerate science. In the past decade, SciSci has benefited from an influx of natural, computational, and social scientists who together have developed big data\textendash{}based capabilities for empirical analysis and generative modeling that capture the unfolding of science, its institutions, and its workforce. The value proposition of SciSci is that with a deeper understanding of the factors that drive successful science, we can more effectively address environmental, societal, and technological problems.
ADVANCESScience can be described as a complex, self-organizing, and evolving network of scholars, projects, papers, and ideas. This representation has unveiled patterns characterizing the emergence of new scientific fields through the study of collaboration networks and the path of impactful discoveries through the study of citation networks. Microscopic models have traced the dynamics of citation accumulation, allowing us to predict the future impact of individual papers. SciSci has revealed choices and trade-offs that scientists face as they advance both their own careers and the scientific horizon. For example, measurements indicate that scholars are risk-averse, preferring to study topics related to their current expertise, which constrains the potential of future discoveries. Those willing to break this pattern engage in riskier careers but become more likely to make major breakthroughs. Overall, the highest-impact science is grounded in conventional combinations of prior work but features unusual combinations. Last, as the locus of research is shifting into teams, SciSci is increasingly focused on the impact of team research, finding that small teams tend to disrupt science and technology with new ideas drawing on older and less prevalent ones. In contrast, large teams tend to develop recent, popular ideas, obtaining high, but often short-lived, impact.
OUTLOOKSciSci offers a deep quantitative understanding of the relational structure between scientists, institutions, and ideas because it facilitates the identification of fundamental mechanisms responsible for scientific discovery. These interdisciplinary data-driven efforts complement contributions from related fields such as scientometrics and the economics and sociology of science. Although SciSci seeks long-standing universal laws and mechanisms that apply across various fields of science, a fundamental challenge going forward is accounting for undeniable differences in culture, habits, and preferences between different fields and countries. This variation makes some cross-domain insights difficult to appreciate and associated science policies difficult to implement. The differences among the questions, data, and skills specific to each discipline suggest that further insights can be gained from domain-specific SciSci studies, which model and identify opportunities adapted to the needs of individual research fields. {$<$}img class={"}fragment-image{"} aria-describedby={"}F1-caption{"} src={"}http://science.sciencemag.org/content/sci/359/6379/eaao0185/F1.medium.gif{"}/{$>$} Download high-res image Open in new tab Download Powerpoint The complexity of science.Science can be seen as an expanding and evolving network of ideas, scholars, and papers. SciSci searches for universal and domain-specific laws underlying the structure and dynamics of science.ILLUSTRATION: NICOLE SAMAY
Identifying fundamental drivers of science and developing predictive models to capture its evolution are instrumental for the design of policies that can improve the scientific enterprise\textemdash{}for example, through enhanced career paths for scientists, better performance evaluation for organizations hosting research, discovery of novel effective funding vehicles, and even identification of promising regions along the scientific frontier. The science of science uses large-scale data on the production of science to search for universal and domain-specific patterns. Here, we review recent developments in this transdisciplinary field.},
  language = {en},
  number = {6379},
  journal = {Science},
  doi = {10.1126/science.aao0185},
  author = {Santo Fortunato and Carl T. Bergstrom and Katy B{\"o}rner and James A. Evans and Dirk Helbing and Sta{\r A}{\textexclamdown}a Milojevi{\a'c} and Alexander M. Petersen and Filippo Radicchi and Roberta Sinatra and Brian Uzzi and Alessandro Vespignani and Ludo Waltman and Dashun Wang and Albert-L{\~A}{\textexclamdown}szl{\~A}{\maththreesuperior} Barab{\a'a}si},
  month = {mar},
  year = {2018},
  pages = {eaao0185},
  file = {C:\\Users\\Usuario\\Zotero\\storage\\P4M4CVD2\\Fortunato et al. - 2018 - Science of science.pdf;C:\\Users\\Usuario\\Zotero\\storage\\XN9SSCBG\\eaao0185.html},
  pmid = {29496846},
}
@Article{olmos-penuelaDoesItTake2016,
  title = {Does It Take Two to Tango? {{Factors}} Related to the Ease of Societal Uptake of Scientific Knowledge},
  volume = {43},
  issn = {0302-3427},
  shorttitle = {Does It Take Two to Tango?},
  abstract = {Abstract.  Science policy increasingly focuses on maximising societal benefits from science and technology investments, but often reduces those benefits to acti},
  language = {en},
  number = {6},
  journal = {Science and Public Policy},
  doi = {10.1093/scipol/scw016},
  author = {Julia Olmos-Pe{\~n}uela and Paul Benneworth and Elena Castro-Mart\'inez},
  month = {dec},
  year = {2016},
  pages = {751-762},
  file = {C:\\Users\\Usuario\\Zotero\\storage\\QMVB2W4B\\Olmos-PeÃ±uela et al. - 2016 - Does it take two to tango Factors related to the .pdf;C:\\Users\\Usuario\\Zotero\\storage\\JYNAW6WG\\2525514.html},
}
